{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "FIRST STEP: dataset loading and preprocessing"
      ],
      "metadata": {
        "id": "sPhP1kVaF2Td"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQQUxQt1FuVm",
        "outputId": "7aec0a08-743b-484a-d2fb-b3f68190923b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 100MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Filtered Train Dataset Size: 5000\n",
            "Filtered Test Dataset Size: 1000\n",
            "Sample Train Batch: Images Shape torch.Size([64, 3, 224, 224]), Labels torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Helper function to filter dataset and load required subsets\n",
        "class FilteredDataset(Dataset):\n",
        "    def __init__(self, dataset, num_per_class):\n",
        "        self.dataset = dataset\n",
        "        self.num_per_class = num_per_class\n",
        "        self.class_counts = [0] * 10\n",
        "        self.filtered_data = []\n",
        "        self._filter_dataset()\n",
        "\n",
        "    def _filter_dataset(self):\n",
        "        for img, label in self.dataset:\n",
        "            if self.class_counts[label] < self.num_per_class:\n",
        "                self.filtered_data.append((img, label))\n",
        "                self.class_counts[label] += 1\n",
        "            if all(count >= self.num_per_class for count in self.class_counts):\n",
        "                break\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.filtered_data[idx]\n",
        "\n",
        "# Load CIFAR-10 dataset + filtering\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "filtered_trainset = FilteredDataset(trainset, num_per_class=500)\n",
        "filtered_testset = FilteredDataset(testset, num_per_class=100)\n",
        "\n",
        "#smaller batches\n",
        "trainloader = DataLoader(filtered_trainset, batch_size=64, shuffle=False, num_workers=0)\n",
        "testloader = DataLoader(filtered_testset, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "# Check dataset sizes\n",
        "print(f\"Filtered Train Dataset Size: {len(filtered_trainset)}\")\n",
        "print(f\"Filtered Test Dataset Size: {len(filtered_testset)}\")\n",
        "\n",
        "# print output to make sure its good\n",
        "train_images, train_labels = next(iter(trainloader))\n",
        "print(f\"Sample Train Batch: Images Shape {train_images.shape}, Labels {train_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction with ResNet-18"
      ],
      "metadata": {
        "id": "Q8n8UhLUIgnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "resnet18 = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
        "resnet18 = resnet18.to(device)\n",
        "resnet18.eval()  # Set model to evaluation mode\n",
        "\n",
        "# features from a batch of images\n",
        "def extract_features(dataloader, model, device):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            # Extract features and flatten the output\n",
        "            output = model(images).view(images.size(0), -1)\n",
        "            features.append(output.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "    # Combine all batches into single arrays\n",
        "    return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "# features for training and test sets\n",
        "train_features, train_labels = extract_features(trainloader, resnet18, device)\n",
        "test_features, test_labels = extract_features(testloader, resnet18, device)\n",
        "\n",
        "print(f\"Train Features Shape: {train_features.shape}\")\n",
        "print(f\"Test Features Shape: {test_features.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzHa1jatIcTS",
        "outputId": "0f22ec05-5828-4e3f-8489-7cc1b1b25d79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 66.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Features Shape: (5000, 512)\n",
            "Test Features Shape: (1000, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality Reduction with PCA"
      ],
      "metadata": {
        "id": "_BHpmw_LIj1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA to make dimensionality 50\n",
        "pca = PCA(n_components=50)\n",
        "train_features_pca = pca.fit_transform(train_features)\n",
        "test_features_pca = pca.transform(test_features)\n",
        "\n",
        "print(f\"Reduced Train Features Shape: {train_features_pca.shape}\")\n",
        "print(f\"Reduced Test Features Shape: {test_features_pca.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrVfns8SIkPF",
        "outputId": "1658c1c8-4ffe-4c64-c28a-78b69abd4d4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced Train Features Shape: (5000, 50)\n",
            "Reduced Test Features Shape: (1000, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAIVE BAISLES"
      ],
      "metadata": {
        "id": "8NVIOsvjLV02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Gaus from scratch\n",
        "class GaussianNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.means = {}\n",
        "        self.variances = {}\n",
        "        self.priors = {}\n",
        "\n",
        "        for cls in self.classes:\n",
        "            X_cls = X[y == cls]\n",
        "            self.means[cls] = np.mean(X_cls, axis=0)\n",
        "            self.variances[cls] = np.var(X_cls, axis=0) + 1e-9  # To avoid division by zero\n",
        "            self.priors[cls] = X_cls.shape[0] / X.shape[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        posteriors = []\n",
        "        for cls in self.classes:\n",
        "            mean, var = self.means[cls], self.variances[cls]\n",
        "            prior = np.log(self.priors[cls])\n",
        "            likelihood = -0.5 * np.sum(np.log(2 * np.pi * var)) - 0.5 * np.sum(((X - mean) ** 2) / var, axis=1)\n",
        "            posterior = prior + likelihood\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmax(posteriors, axis=0)]\n",
        "\n",
        "# Train\n",
        "gnb_scratch = GaussianNaiveBayes()\n",
        "gnb_scratch.fit(train_features_pca, train_labels)\n",
        "\n",
        "# Predict\n",
        "test_preds_scratch = gnb_scratch.predict(test_features_pca)\n",
        "\n",
        "# See results and print\n",
        "accuracy_scratch = accuracy_score(test_labels, test_preds_scratch)\n",
        "precision_scratch = precision_score(test_labels, test_preds_scratch, average='macro')\n",
        "recall_scratch = recall_score(test_labels, test_preds_scratch, average='macro')\n",
        "f1_scratch = f1_score(test_labels, test_preds_scratch, average='macro')\n",
        "conf_matrix_scratch = confusion_matrix(test_labels, test_preds_scratch)\n",
        "\n",
        "print(\"Custom Naive Bayes Results:\")\n",
        "print(f\"Accuracy: {accuracy_scratch:.2f}\")\n",
        "print(f\"Precision: {precision_scratch:.2f}\")\n",
        "print(f\"Recall: {recall_scratch:.2f}\")\n",
        "print(f\"F1 Score: {f1_scratch:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_scratch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train scikit-learn's GaussianNB\n",
        "gnb_sklearn = GaussianNB()\n",
        "gnb_sklearn.fit(train_features_pca, train_labels)\n",
        "\n",
        "# Predict\n",
        "test_preds_sklearn = gnb_sklearn.predict(test_features_pca)\n",
        "\n",
        "# See results and print\n",
        "accuracy_sklearn = accuracy_score(test_labels, test_preds_sklearn)\n",
        "precision_sklearn = precision_score(test_labels, test_preds_sklearn, average='macro')\n",
        "recall_sklearn = recall_score(test_labels, test_preds_sklearn, average='macro')\n",
        "f1_sklearn = f1_score(test_labels, test_preds_sklearn, average='macro')\n",
        "conf_matrix_sklearn = confusion_matrix(test_labels, test_preds_sklearn)\n",
        "\n",
        "print(\"\\nScikit-learn Naive Bayes Results:\")\n",
        "print(f\"Accuracy: {accuracy_sklearn:.2f}\")\n",
        "print(f\"Precision: {precision_sklearn:.2f}\")\n",
        "print(f\"Recall: {recall_sklearn:.2f}\")\n",
        "print(f\"F1 Score: {f1_sklearn:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_sklearn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAVCVr08LVfn",
        "outputId": "0e095bd7-25df-4669-8122-6e38b45442e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Naive Bayes Results:\n",
            "Accuracy: 0.79\n",
            "Precision: 0.80\n",
            "Recall: 0.79\n",
            "F1 Score: 0.79\n",
            "Confusion Matrix:\n",
            "[[80  1  1  1  0  0  1  0 12  4]\n",
            " [ 3 88  0  2  1  0  0  0  0  6]\n",
            " [ 7  0 62  8  7  4 11  0  1  0]\n",
            " [ 1  0  4 75  4 10  6  0  0  0]\n",
            " [ 1  0  4  7 77  3  1  7  0  0]\n",
            " [ 0  1  5 15  3 73  2  1  0  0]\n",
            " [ 2  0  4  6  6  3 78  1  0  0]\n",
            " [ 1  1  0  5  6  5  0 81  1  0]\n",
            " [ 8  0  1  0  1  0  0  0 87  3]\n",
            " [ 5  2  0  2  0  0  0  1  1 89]]\n",
            "\n",
            "Scikit-learn Naive Bayes Results:\n",
            "Accuracy: 0.79\n",
            "Precision: 0.80\n",
            "Recall: 0.79\n",
            "F1 Score: 0.79\n",
            "Confusion Matrix:\n",
            "[[80  1  1  1  0  0  1  0 12  4]\n",
            " [ 3 88  0  2  1  0  0  0  0  6]\n",
            " [ 7  0 62  8  7  4 11  0  1  0]\n",
            " [ 1  0  4 75  4 10  6  0  0  0]\n",
            " [ 1  0  4  7 77  3  1  7  0  0]\n",
            " [ 0  1  5 15  3 73  2  1  0  0]\n",
            " [ 2  0  4  6  6  3 78  1  0  0]\n",
            " [ 1  1  0  5  6  5  0 81  1  0]\n",
            " [ 8  0  1  0  1  0  0  0 87  3]\n",
            " [ 5  2  0  2  0  0  0  1  1 89]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees + Experimentation Changing depths"
      ],
      "metadata": {
        "id": "vCC0sKaaMEfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# using Gini impurity\n",
        "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=50, random_state=42)\n",
        "decision_tree.fit(train_features_pca, train_labels)\n",
        "\n",
        "# Predict\n",
        "test_preds_tree = decision_tree.predict(test_features_pca)\n",
        "\n",
        "# See results and print\n",
        "accuracy_tree = accuracy_score(test_labels, test_preds_tree)\n",
        "precision_tree = precision_score(test_labels, test_preds_tree, average='macro')\n",
        "recall_tree = recall_score(test_labels, test_preds_tree, average='macro')\n",
        "f1_tree = f1_score(test_labels, test_preds_tree, average='macro')\n",
        "conf_matrix_tree = confusion_matrix(test_labels, test_preds_tree)\n",
        "\n",
        "print(\"Default Decision Tree Results (Max Depth = 50):\")\n",
        "print(f\"Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Precision: {precision_tree:.2f}\")\n",
        "print(f\"Recall: {recall_tree:.2f}\")\n",
        "print(f\"F1 Score: {f1_tree:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_tree)\n",
        "\n",
        "# Changing tree depths\n",
        "depths = [10, 20, 30, 40, 50]\n",
        "depth_results = []\n",
        "\n",
        "print(\"\\nDecision Tree Depth Experiments:\")\n",
        "for depth in depths:\n",
        "    tree = DecisionTreeClassifier(criterion='gini', max_depth=depth, random_state=42)\n",
        "    tree.fit(train_features_pca, train_labels)\n",
        "    preds = tree.predict(test_features_pca)\n",
        "\n",
        "    acc = accuracy_score(test_labels, preds)\n",
        "    prec = precision_score(test_labels, preds, average='macro')\n",
        "    rec = recall_score(test_labels, preds, average='macro')\n",
        "    f1 = f1_score(test_labels, preds, average='macro')\n",
        "    conf_matrix = confusion_matrix(test_labels, preds)\n",
        "\n",
        "    depth_results.append((depth, acc, prec, rec, f1, conf_matrix))\n",
        "\n",
        "    print(f\"\\nDepth: {depth}\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Precision: {prec:.2f}\")\n",
        "    print(f\"Recall: {rec:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz5b1UsrL-7y",
        "outputId": "b9491711-6fa6-4bd2-fd5c-cf1b8c5c3829"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Decision Tree Results (Max Depth = 50):\n",
            "Accuracy: 0.58\n",
            "Precision: 0.59\n",
            "Recall: 0.59\n",
            "F1 Score: 0.58\n",
            "Confusion Matrix:\n",
            "[[56  4  8  4  3  0  1  1 19  4]\n",
            " [ 7 68  0  4  0  1  0  0  7 13]\n",
            " [ 3  1 42 11  8 10 17  7  1  0]\n",
            " [ 1  0  9 44  3 23 13  4  1  2]\n",
            " [ 5  1 12  6 45  7  4 18  0  2]\n",
            " [ 0  0  8 14  6 60  6  4  2  0]\n",
            " [ 1  0 11  5  3  2 76  2  0  0]\n",
            " [ 1  1  6 11 10 14  0 55  1  1]\n",
            " [22  3  2  0  0  0  2  1 65  5]\n",
            " [ 5  8  0  1  1  0  0  1 10 74]]\n",
            "\n",
            "Decision Tree Depth Experiments:\n",
            "\n",
            "Depth: 10\n",
            "Accuracy: 0.61\n",
            "Precision: 0.62\n",
            "Recall: 0.61\n",
            "F1 Score: 0.61\n",
            "Confusion Matrix:\n",
            "[[55  7  7  6  3  1  0  0 17  4]\n",
            " [ 6 70  0  3  0  2  2  0  5 12]\n",
            " [ 4  1 46 19  4 10 11  4  1  0]\n",
            " [ 1  0  9 61  4 17  6  1  0  1]\n",
            " [ 6  0  5  6 57 10  3 12  0  1]\n",
            " [ 0  0  6 25  4 57  3  3  2  0]\n",
            " [ 3  0 11 11  4  1 69  1  0  0]\n",
            " [ 1  1  4 13 11 13  0 55  1  1]\n",
            " [18  5  2  1  0  0  1  1 68  4]\n",
            " [ 7  9  0  2  1  0  0  1  8 72]]\n",
            "\n",
            "Depth: 20\n",
            "Accuracy: 0.59\n",
            "Precision: 0.59\n",
            "Recall: 0.59\n",
            "F1 Score: 0.59\n",
            "Confusion Matrix:\n",
            "[[55  5  8  4  3  0  1  1 19  4]\n",
            " [ 6 71  0  5  0  1  0  0  8  9]\n",
            " [ 3  1 42 11  8 10 17  7  1  0]\n",
            " [ 1  0  9 44  3 23 13  5  1  1]\n",
            " [ 5  1 12  6 45  7  4 18  0  2]\n",
            " [ 0  0  9 14  6 60  5  4  2  0]\n",
            " [ 1  0 11  4  4  2 76  2  0  0]\n",
            " [ 1  1  6 11 10 14  0 55  1  1]\n",
            " [22  2  2  0  0  0  2  1 64  7]\n",
            " [ 5  9  0  1  1  0  0  1  9 74]]\n",
            "\n",
            "Depth: 30\n",
            "Accuracy: 0.58\n",
            "Precision: 0.59\n",
            "Recall: 0.59\n",
            "F1 Score: 0.58\n",
            "Confusion Matrix:\n",
            "[[56  4  8  4  3  0  1  1 19  4]\n",
            " [ 7 68  0  4  0  1  0  0  7 13]\n",
            " [ 3  1 42 11  8 10 17  7  1  0]\n",
            " [ 1  0  9 44  3 23 13  4  1  2]\n",
            " [ 5  1 12  6 45  7  4 18  0  2]\n",
            " [ 0  0  8 14  6 60  6  4  2  0]\n",
            " [ 1  0 11  5  3  2 76  2  0  0]\n",
            " [ 1  1  6 11 10 14  0 55  1  1]\n",
            " [22  3  2  0  0  0  2  1 65  5]\n",
            " [ 5  8  0  1  1  0  0  1 10 74]]\n",
            "\n",
            "Depth: 40\n",
            "Accuracy: 0.58\n",
            "Precision: 0.59\n",
            "Recall: 0.59\n",
            "F1 Score: 0.58\n",
            "Confusion Matrix:\n",
            "[[56  4  8  4  3  0  1  1 19  4]\n",
            " [ 7 68  0  4  0  1  0  0  7 13]\n",
            " [ 3  1 42 11  8 10 17  7  1  0]\n",
            " [ 1  0  9 44  3 23 13  4  1  2]\n",
            " [ 5  1 12  6 45  7  4 18  0  2]\n",
            " [ 0  0  8 14  6 60  6  4  2  0]\n",
            " [ 1  0 11  5  3  2 76  2  0  0]\n",
            " [ 1  1  6 11 10 14  0 55  1  1]\n",
            " [22  3  2  0  0  0  2  1 65  5]\n",
            " [ 5  8  0  1  1  0  0  1 10 74]]\n",
            "\n",
            "Depth: 50\n",
            "Accuracy: 0.58\n",
            "Precision: 0.59\n",
            "Recall: 0.59\n",
            "F1 Score: 0.58\n",
            "Confusion Matrix:\n",
            "[[56  4  8  4  3  0  1  1 19  4]\n",
            " [ 7 68  0  4  0  1  0  0  7 13]\n",
            " [ 3  1 42 11  8 10 17  7  1  0]\n",
            " [ 1  0  9 44  3 23 13  4  1  2]\n",
            " [ 5  1 12  6 45  7  4 18  0  2]\n",
            " [ 0  0  8 14  6 60  6  4  2  0]\n",
            " [ 1  0 11  5  3  2 76  2  0  0]\n",
            " [ 1  1  6 11 10 14  0 55  1  1]\n",
            " [22  3  2  0  0  0  2  1 65  5]\n",
            " [ 5  8  0  1  1  0  0  1 10 74]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Model"
      ],
      "metadata": {
        "id": "BLBYCn8MMqu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(50, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batchnorm = nn.BatchNorm1d(512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.batchnorm(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# loss function, and optimizer\n",
        "model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# data prep\n",
        "train_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Train epoch loops\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i in range(0, len(train_tensor), batch_size):\n",
        "        inputs = train_tensor[i:i+batch_size]\n",
        "        labels = train_labels_tensor[i:i+batch_size]\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # make sure program is running iwth this\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_tensor)\n",
        "    _, test_preds = torch.max(test_outputs, 1)\n",
        "\n",
        "# predictions need to be converted to numpy to calculate\n",
        "test_preds_np = test_preds.cpu().numpy()\n",
        "test_labels_np = test_labels_tensor.cpu().numpy()\n",
        "\n",
        "# See results and print\n",
        "accuracy_mlp = accuracy_score(test_labels_np, test_preds_np)\n",
        "precision_mlp = precision_score(test_labels_np, test_preds_np, average='macro')\n",
        "recall_mlp = recall_score(test_labels_np, test_preds_np, average='macro')\n",
        "f1_mlp = f1_score(test_labels_np, test_preds_np, average='macro')\n",
        "conf_matrix_mlp = confusion_matrix(test_labels_np, test_preds_np)\n",
        "\n",
        "print(f\"MLP Accuracy: {accuracy_mlp:.2f}\")\n",
        "print(f\"Precision: {precision_mlp:.2f}\")\n",
        "print(f\"Recall: {recall_mlp:.2f}\")\n",
        "print(f\"F1 Score: {f1_mlp:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_mlp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcIWEF1RMrBr",
        "outputId": "ff28dafa-b743-4eab-8e33-1b8bc7c79fb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 4.6075\n",
            "Epoch [2/20], Loss: 2.7075\n",
            "Epoch [3/20], Loss: 1.7726\n",
            "Epoch [4/20], Loss: 1.1632\n",
            "Epoch [5/20], Loss: 1.0814\n",
            "Epoch [6/20], Loss: 0.7836\n",
            "Epoch [7/20], Loss: 0.6552\n",
            "Epoch [8/20], Loss: 0.5032\n",
            "Epoch [9/20], Loss: 0.2960\n",
            "Epoch [10/20], Loss: 0.1984\n",
            "Epoch [11/20], Loss: 0.0992\n",
            "Epoch [12/20], Loss: 0.0464\n",
            "Epoch [13/20], Loss: 0.0360\n",
            "Epoch [14/20], Loss: 0.0316\n",
            "Epoch [15/20], Loss: 0.0285\n",
            "Epoch [16/20], Loss: 0.0260\n",
            "Epoch [17/20], Loss: 0.0240\n",
            "Epoch [18/20], Loss: 0.0221\n",
            "Epoch [19/20], Loss: 0.0206\n",
            "Epoch [20/20], Loss: 0.0193\n",
            "MLP Accuracy: 0.82\n",
            "Precision: 0.83\n",
            "Recall: 0.82\n",
            "F1 Score: 0.83\n",
            "Confusion Matrix:\n",
            "[[80  0  5  1  0  0  2  0  8  4]\n",
            " [ 3 86  0  2  0  0  1  0  2  6]\n",
            " [ 4  0 76  4  3  4  6  1  2  0]\n",
            " [ 0  0  3 76  3  8  9  0  1  0]\n",
            " [ 2  0  2  7 80  3  0  6  0  0]\n",
            " [ 0  0  6 14  2 73  3  1  1  0]\n",
            " [ 1  0  0  3  5  1 89  1  0  0]\n",
            " [ 1  0  1  5  7  4  0 82  0  0]\n",
            " [ 5  1  1  0  0  0  0  1 92  0]\n",
            " [ 2  3  0  1  0  0  0  0  3 91]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Varying the Depth"
      ],
      "metadata": {
        "id": "zTOBVjPCWRrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXRRA LAYER THATS HIDDEN PART TEST\n",
        "\n",
        "class MLP_Deep(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_Deep, self).__init__()\n",
        "        self.fc1 = nn.Linear(50, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.batchnorm1(self.fc2(x)))\n",
        "        x = self.relu(self.batchnorm2(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# train + eval\n",
        "def train_and_evaluate(model, train_features, train_labels, test_features, test_labels, num_epochs=20):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    train_tensor = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
        "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "    test_tensor = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "    # expoch loops\n",
        "    batch_size = 64\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i in range(0, len(train_tensor), batch_size):\n",
        "            inputs = train_tensor[i:i+batch_size]\n",
        "            labels = train_labels_tensor[i:i+batch_size]\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # make sure program is running\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(test_tensor)\n",
        "        _, test_preds = torch.max(test_outputs, 1)\n",
        "\n",
        "    test_preds_np = test_preds.cpu().numpy()\n",
        "    test_labels_np = test_labels_tensor.cpu().numpy()\n",
        "\n",
        "    # convert like before\n",
        "    accuracy = accuracy_score(test_labels_np, test_preds_np)\n",
        "    precision = precision_score(test_labels_np, test_preds_np, average='macro')\n",
        "    recall = recall_score(test_labels_np, test_preds_np, average='macro')\n",
        "    f1 = f1_score(test_labels_np, test_preds_np, average='macro')\n",
        "    conf_matrix = confusion_matrix(test_labels_np, test_preds_np)\n",
        "# See results and print\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "# call all functions\n",
        "print(\"Training the deeper MLP model...\")\n",
        "mlp_deep = MLP_Deep()\n",
        "train_and_evaluate(mlp_deep, train_features_pca, train_labels, test_features_pca, test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RnQF1gtWTC8",
        "outputId": "062fa240-1091-4e6e-e9f6-49e50402826f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the deeper MLP model...\n",
            "Epoch [1/20], Loss: 4.2734\n",
            "Epoch [2/20], Loss: 2.9002\n",
            "Epoch [3/20], Loss: 1.9091\n",
            "Epoch [4/20], Loss: 1.2495\n",
            "Epoch [5/20], Loss: 0.8969\n",
            "Epoch [6/20], Loss: 0.7998\n",
            "Epoch [7/20], Loss: 0.4060\n",
            "Epoch [8/20], Loss: 0.1655\n",
            "Epoch [9/20], Loss: 0.0903\n",
            "Epoch [10/20], Loss: 0.0706\n",
            "Epoch [11/20], Loss: 0.0591\n",
            "Epoch [12/20], Loss: 0.0517\n",
            "Epoch [13/20], Loss: 0.0462\n",
            "Epoch [14/20], Loss: 0.0421\n",
            "Epoch [15/20], Loss: 0.0387\n",
            "Epoch [16/20], Loss: 0.0359\n",
            "Epoch [17/20], Loss: 0.0335\n",
            "Epoch [18/20], Loss: 0.0315\n",
            "Epoch [19/20], Loss: 0.0297\n",
            "Epoch [20/20], Loss: 0.0282\n",
            "Accuracy: 0.83\n",
            "Precision: 0.83\n",
            "Recall: 0.83\n",
            "F1 Score: 0.83\n",
            "Confusion Matrix:\n",
            "[[81  0  5  2  1  0  0  0  8  3]\n",
            " [ 2 87  0  1  0  0  0  0  3  7]\n",
            " [ 4  0 80  5  2  3  4  1  1  0]\n",
            " [ 0  0  7 70  5  9  7  1  1  0]\n",
            " [ 2  0  5  4 78  3  1  7  0  0]\n",
            " [ 0  0  5 15  3 73  2  1  1  0]\n",
            " [ 1  0  5  1  2  2 88  1  0  0]\n",
            " [ 1  0  1  3  6  2  0 87  0  0]\n",
            " [ 4  0  1  1  0  0  0  0 93  1]\n",
            " [ 2  1  0  1  0  1  0  0  4 91]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vary Hidden Layer Sizes"
      ],
      "metadata": {
        "id": "VaaQEEINWUHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SMALLER HIDDEN LAYERS HIDDEN TEST\n",
        "class MLP_Smaller(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_Smaller, self).__init__()\n",
        "        self.fc1 = nn.Linear(50, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batchnorm1 = nn.BatchNorm1d(256)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batchnorm1(self.fc1(x)))\n",
        "        x = self.relu(self.batchnorm2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# See results and print\n",
        "print(\"\\nTraining the smaller MLP model...\")\n",
        "mlp_smaller = MLP_Smaller()\n",
        "train_and_evaluate(mlp_smaller, train_features_pca, train_labels, test_features_pca, test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC3ps7_pWYxL",
        "outputId": "2b72fddc-c593-468a-97c7-4b3c720aa539"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the smaller MLP model...\n",
            "Epoch [1/20], Loss: 3.8533\n",
            "Epoch [2/20], Loss: 2.6254\n",
            "Epoch [3/20], Loss: 1.9595\n",
            "Epoch [4/20], Loss: 1.5138\n",
            "Epoch [5/20], Loss: 1.1935\n",
            "Epoch [6/20], Loss: 0.9957\n",
            "Epoch [7/20], Loss: 0.9294\n",
            "Epoch [8/20], Loss: 0.8816\n",
            "Epoch [9/20], Loss: 0.6595\n",
            "Epoch [10/20], Loss: 0.5073\n",
            "Epoch [11/20], Loss: 0.3525\n",
            "Epoch [12/20], Loss: 0.3024\n",
            "Epoch [13/20], Loss: 0.2333\n",
            "Epoch [14/20], Loss: 0.1714\n",
            "Epoch [15/20], Loss: 0.1269\n",
            "Epoch [16/20], Loss: 0.1004\n",
            "Epoch [17/20], Loss: 0.0857\n",
            "Epoch [18/20], Loss: 0.0755\n",
            "Epoch [19/20], Loss: 0.0683\n",
            "Epoch [20/20], Loss: 0.0624\n",
            "Accuracy: 0.82\n",
            "Precision: 0.82\n",
            "Recall: 0.82\n",
            "F1 Score: 0.82\n",
            "Confusion Matrix:\n",
            "[[81  0  7  0  1  0  0  1  9  1]\n",
            " [ 2 85  1  1  0  0  1  0  3  7]\n",
            " [ 4  0 77  7  4  1  6  1  0  0]\n",
            " [ 0  0  3 72  5 12  7  1  0  0]\n",
            " [ 2  0  2  6 80  2  2  5  1  0]\n",
            " [ 0  0  4 19  2 70  2  2  1  0]\n",
            " [ 1  0  1  2  3  2 90  1  0  0]\n",
            " [ 0  0  2  5  9  6  0 78  0  0]\n",
            " [ 5  0  1  1  0  0  0  0 93  0]\n",
            " [ 2  1  0  1  0  0  1  0  4 91]]\n"
          ]
        }
      ]
    }
  ]
}